import Image from 'next/image';

<div className="flex flex-col items-center justify-center mb-8">
    <Image
        src="./carbon_pyesl.png"
        alt="depparse-grammar"
        width={300}
        height={225}
        className="w-64 h-auto mb-4 md:w-96 md:h-auto"
    />
</div>

# [depparse-grammar](https://github.com/skarokin/depparse-grammar)
### Python, spaCy, React, Firebase
- A customizable grammar checker that combines a dependency parser with rule-based systems.
- Leverages spaCy training pipelines to support multiple languages and custom grammar rules.
- Inspired by [this](https://mmozgovoy.dev/papers/mozgovoy11b.pdf) research paper.
<br/>
### Blog
- Started out as random research on HMMs, which led me to HMMs for POS tagging.
- Came up with an idea to use an HMM for grammar checking, realized there was a major flaw in my plan.
- Discovery of major flaw led me to find out about dependency parsers. Did a whole bunch of research on them and wanted to make one from scratch.
- Decided to use an existing one for grammar checking since I couldn't understand any of those research papers. Played around with spaCy's 
dependency parser and realized it made wrong predictions on ungrammatical sentences.
- Found the research paper linked above, got in touch with the researcher and got a lot of helpful insight.
- Got OntoNotes 5.0 corpus but realized it was in PTB not CoNLL-U.
- Spent 2 weeks trying to get an old PTB to CoNLL-U script working then found out that Stanford had an actively maintained converter... lol!! Made a tool to deeply traverse a data directory and convert from PTB to CoNLL-U.
- Made a little multithreaded tool to convert CoNLL-U to spaCy binary (this was pretty simple since it was just a list of command line scripts).
- Found a nice [tool](https://pypi.org/project/word-forms/) that generates all different forms of a word. Combined this tool with NLTK to ensure the word is valid and 
spaCy to generate the UD POS tag of the word. This capability to generate different forms of a word serves as the basis for the augmentor.
- Augmented the dataset by replacing adverbs with adjectives (and vice versa) and injecting all kinds of wrong verb forms
    - The pretrained model was already capable of SVA, auxiliary verbs, and some other grammar errors so they did not need to be introduced.
    - Implemented multithreading and multiprocessing; augmenting speed reduced from 220s to 90s for the entire OntoNotes 5.0 dataset
        - NLTK's wordnet is not thread-safe, so I had to do a little modification to the tool above. My fork for this is [here](https://github.com/skarokin/word_forms_threadsafe)
